{
  "model_name": "meta-llama/Llama-3.2-1B",
  "max_length": 2048,
  "device": "cuda",
  "head_aggregation": "attention_weighted",
  "symmetrization": "row_norm",
  "normalization": "sym",
  "hfer_cutoff_ratio": 0.1,
  "num_eigenvalues": 100,
  "eigen_solver": "sparse",
  "lanczos_max_iter": 1000,
  "batch_size": 1,
  "num_layers_analyze": null,
  "save_attention": true,
  "save_activations": true,
  "output_dir": ".\\iclr_results\\multi20_expanded\\llama-3.2-1b\\en_active\\sym-row_norm\\agg-attention_weighted\\run_9",
  "save_plots": true,
  "save_intermediate": false,
  "verbose": false
}