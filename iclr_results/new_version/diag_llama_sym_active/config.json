{
  "model_name": "meta-llama/Llama-3.2-1B",
  "max_length": 2048,
  "device": "cuda",
  "head_aggregation": "uniform",
  "symmetrization": "symmetric",
  "normalization": "sym",
  "hfer_cutoff_ratio": 0.1,
  "num_eigenvalues": 100,
  "eigen_solver": "sparse",
  "lanczos_max_iter": 1000,
  "batch_size": 1,
  "num_layers_analyze": null,
  "save_attention": true,
  "save_activations": true,
  "output_dir": ".\\iclr_results\\new_version\\diag_llama_sym_active",
  "save_plots": true,
  "save_intermediate": false,
  "verbose": true
}